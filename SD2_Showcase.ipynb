{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check type of GPU and VRAM available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi --query-gpu=name,memory.total,memory.free --format=csv,noheader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Requirements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clone repos we need\n",
    "!git clone https://github.com/Jan-Oliver/profaile-pic-dev.git\n",
    "!git clone https://github.com/huggingface/diffusers.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle all of the dependencies\n",
    "!cd /content/profaile-pic-dev && chmod +x dreambooth/setup_dreambooth_env.sh\n",
    "!cd /content/profaile-pic-dev && dreambooth/setup_dreambooth_env.sh\n",
    "!cd /content/profaile-pic-dev && chmod +x inference/setup_inference_env.sh\n",
    "!cd /content/profaile-pic-dev && inference/setup_inference_env.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect to GDrive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Just leave everything as it is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.10.8 64-bit' requires ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/opt/homebrew/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# Store model weights here\n",
    "OUTPUT_DIR = \"/content/stable_diffusion_weights/train\"\n",
    "# We will use prior preservation\n",
    "CLASS_IMAGES_DIR = None\n",
    "# 2.1 with 512x512 resolution \n",
    "MODEL_NAME = \"stabilityai/stable-diffusion-2-1-base\"\n",
    "# Use Floating Point 16 -> Reduce VRAM of GPU\n",
    "PRECISION = \"fp16\"\n",
    "# Make folders\n",
    "!mkdir -p $INSTANCE_IMAGES_DIR_DRIVE\n",
    "!mkdir -p $OUTPUT_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adapt this:\n",
    "- YOUR_NAME_ABBREVIATION: Simply use the first letter of your first name and then your last name.\n",
    "- MALE_OR_FEMALE: If you are a man, set this to man. If you are a woman, set this to woman.\n",
    "\n",
    "\n",
    "**Why do we need this**?:\n",
    "- This is based on the Dreambooth Paper. You can find it [here](https://dreambooth.github.io)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "YOUR_NAME_ABBREVIATION = \"jseidenfuss\"\n",
    "MAN_OR_WOMAN = \"man\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concepts_list = [\n",
    "    {\n",
    "        \"instance_prompt\":      f\"photo of {YOUR_NAME_ABBREVIATION} {MAN_OR_WOMAN}\",\n",
    "        \"class_prompt\":         f\"photo of a {MAN_OR_WOMAN}\",\n",
    "        \"instance_data_dir\":    \"/content/data/instance_images\",\n",
    "        \"class_data_dir\":       \"/content/data/class_images\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# `class_data_dir` contains regularization images\n",
    "import json\n",
    "import os\n",
    "for c in concepts_list:\n",
    "    os.makedirs(c[\"instance_data_dir\"], exist_ok=True)\n",
    "    os.makedirs(c[\"class_data_dir\"], exist_ok=True)\n",
    "\n",
    "with open(\"concepts_list.json\", \"w\") as f:\n",
    "    json.dump(concepts_list, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now upload 10 - 15 images into /data/instance_data_dir\n",
    "- They all have to be cropped to 512x512. \n",
    "- To do that use [this mass cropping tool](https://www.birme.net)\n",
    "- Make sure your it is only you on the images and you have quite a bit of variable poses, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Specify the amount of images you uploaded**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_IMAGES = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Memory consumption\n",
    "\n",
    "Use the table below to choose the best flags based on your memory and speed requirements. Tested on Tesla T4 GPU.\n",
    "\n",
    "\n",
    "| `fp16` | `train_batch_size` | `gradient_accumulation_steps` | `gradient_checkpointing` | `use_8bit_adam` | GB VRAM usage | Speed (it/s) |\n",
    "| ---- | ------------------ | ----------------------------- | ----------------------- | --------------- | ---------- | ------------ |\n",
    "| fp16 | 1                  | 1                             | TRUE                    | TRUE            | 9.92       | 0.93         |\n",
    "| no   | 1                  | 1                             | TRUE                    | TRUE            | 10.08      | 0.42         |\n",
    "| fp16 | 2                  | 1                             | TRUE                    | TRUE            | 10.4       | 0.66         |\n",
    "| fp16 | 1                  | 1                             | FALSE                   | TRUE            | 11.17      | 1.14         |\n",
    "| no   | 1                  | 1                             | FALSE                   | TRUE            | 11.17      | 0.49         |\n",
    "| fp16 | 1                  | 2                             | TRUE                    | TRUE            | 11.56      | 1            |\n",
    "| fp16 | 2                  | 1                             | FALSE                   | TRUE            | 13.67      | 0.82         |\n",
    "| fp16 | 1                  | 2                             | FALSE                   | TRUE            | 13.7       | 0.83          |\n",
    "| fp16 | 1                  | 1                             | TRUE                    | FALSE           | 15.79      | 0.77         |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add `--gradient_checkpointing` flag for around 9.92 GB VRAM usage.\n",
    "\n",
    "remove `--use_8bit_adam` flag for full precision. Requires 15.79 GB with `--gradient_checkpointing` else 17.8 GB.\n",
    "\n",
    "remove `--train_text_encoder` flag to reduce memory usage further, degrades output quality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Launch the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Dont change\n",
    "NUM_CLASS_IMAGES = N_IMAGES * 12\n",
    "MAX_NUM_STEPS = N_IMAGES * 80\n",
    "LR_WARMUP_STEPS = int(MAX_NUM_STEPS / 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!accelerate launch /content/profaile-pic-dev/dreambooth/train_dreambooth.py \\\n",
    "  --pretrained_model_name_or_path=$MODEL_NAME \\\n",
    "  --pretrained_vae_name_or_path=\"stabilityai/sd-vae-ft-mse\" \\\n",
    "  --output_dir=$OUTPUT_DIR \\\n",
    "  --with_prior_preservation --prior_loss_weight=1.0 \\\n",
    "  --seed=1337 \\\n",
    "  --resolution=512 \\\n",
    "  --train_batch_size=1 \\\n",
    "  --train_text_encoder \\\n",
    "  --mixed_precision=$PRECISION \\\n",
    "  --gradient_accumulation_steps=1 \\\n",
    "  --learning_rate=1e-6 \\\n",
    "  --lr_scheduler=\"polynomial\" \\\n",
    "  --lr_warmup_steps=$LR_WARMUP_STEPS \\\n",
    "  --num_class_images=$NUM_CLASS_IMAGES \\\n",
    "  --sample_batch_size=4 \\\n",
    "  --max_train_steps=$MAX_NUM_STEPS \\\n",
    "  --save_interval=4000 \\\n",
    "  --save_min_steps=4000 \\\n",
    "  --save_sample_prompt= f\"medium shot side profile portrait photo of the {YOUR_NAME_ABBREVIATION} {MAN_OR_WOMAN} warrior chief, tribal panther make up, blue on red, looking away, serious eyes, 50mm portrait, photography, hard rim lighting photography –ar 2:3 –beta –upbeta\" \\\n",
    "  --save_sample_negative_prompt=\"blender, ugly, multiple hands, bad anatomy, bad proportions, unrealistic, full body, cropped, lowres, poorly drawn face, out of frame, poorly drawn hands, double, blurred, disfigured, deformed, repetitive, black and white\" \\\n",
    "  --n_save_sample=4 \\\n",
    "  --save_guidance_scale=7.5 \\\n",
    "  --save_infer_steps=50 \\\n",
    "  --concepts_list=\"concepts_list.json\" \\\n",
    "  --wandb_group_name=$GROUP_NAME \\\n",
    "  --wandb_project_name=$PROJECT_NAME  \\\n",
    "  --use_8bit_adam\n",
    "\n",
    "# Reduce the `--save_interval` to lower than `--max_train_steps` to save weights from intermediate steps.\n",
    "# `--save_sample_prompt` can be same as `--instance_prompt` to generate intermediate samples (saved along with weights in samples directory)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Specify the weights directory to use (leave blank for latest)\n",
    "WEIGHTS_DIR = os.path.join(OUTPUT_DIR, \"2250\")\n",
    "print(f\"[*] WEIGHTS_DIR={WEIGHTS_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preview the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "weights_folder = OUTPUT_DIR\n",
    "folders = sorted([f for f in os.listdir(weights_folder) if f != \"0\"], key=lambda x: int(x))\n",
    "\n",
    "row = len(folders)\n",
    "col = len(os.listdir(os.path.join(weights_folder, folders[0], \"samples\")))\n",
    "scale = 4\n",
    "fig, axes = plt.subplots(row, col, figsize=(col*scale, row*scale), gridspec_kw={'hspace': 0, 'wspace': 0})\n",
    "\n",
    "for i, folder in enumerate(folders):\n",
    "    folder_path = os.path.join(weights_folder, folder)\n",
    "    image_folder = os.path.join(folder_path, \"samples\")\n",
    "    images = [f for f in os.listdir(image_folder)]\n",
    "    for j, image in enumerate(images):\n",
    "        if row == 1:\n",
    "            currAxes = axes[j]\n",
    "        else:\n",
    "            currAxes = axes[i, j]\n",
    "        if i == 0:\n",
    "            currAxes.set_title(f\"Image {j}\")\n",
    "        if j == 0:\n",
    "            currAxes.text(-0.1, 0.5, folder, rotation=0, va='center', ha='center', transform=currAxes.transAxes)\n",
    "        image_path = os.path.join(image_folder, image)\n",
    "        img = mpimg.imread(image_path)\n",
    "        currAxes.imshow(img, cmap='gray')\n",
    "        currAxes.axis('off')\n",
    "        \n",
    "plt.tight_layout()\n",
    "plt.savefig('grid.png', dpi=72)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Push model to Google Drive so you can use it later\n",
    "\n",
    "- Adapt this path: PATH_TO_COPY_MODEL_TO_GDRIVE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "PATH_TO_COPY_MODEL_TO_GDRIVE = \"\"\n",
    "shutil.copytree(WEIGHTS_DIR, )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 Inference\n",
    "\n",
    "### Util stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Util functions\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch import autocast\n",
    "from diffusers import StableDiffusionPipeline, DDIMScheduler, EulerDiscreteScheduler\n",
    "from IPython.display import display\n",
    "\n",
    "if PRECISION == \"fp16\":\n",
    "    fp16 = True\n",
    "else:\n",
    "    fp16 = False\n",
    "\n",
    "def image_grid(imgs, rows, cols):\n",
    "    assert len(imgs) == rows*cols\n",
    "\n",
    "    w, h = imgs[0].size\n",
    "    grid = Image.new('RGB', size=(cols*w, rows*h))\n",
    "    grid_w, grid_h = grid.size\n",
    "    \n",
    "    for i, img in enumerate(imgs):\n",
    "        grid.paste(img, box=(i%cols*w, i//cols*h))\n",
    "    return grid\n",
    "\n",
    "model_path = WEIGHTS_DIR             # If you want to use previously trained model saved in gdrive, replace this with the full path of model in gdrive\n",
    "\n",
    "scheduler_euler = EulerDiscreteScheduler.from_pretrained(MODEL_NAME, subfolder=\"scheduler\")\n",
    "scheduler_ddim = DDIMScheduler.from_pretrained(MODEL_NAME,subfolder=\"scheduler\")\n",
    "\n",
    "# First for Euler\n",
    "if fp16:\n",
    "  pipe = StableDiffusionPipeline.from_pretrained(model_path, scheduler=scheduler_euler, safety_checker=None, torch_dtype=torch.float16).to(\"cuda\")\n",
    "else:\n",
    "  pipe = StableDiffusionPipeline.from_pretrained(model_path, scheduler=scheduler_euler, safety_checker=None, torch_dtype=torch.float32).to(\"cuda\")\n",
    "\n",
    "# Then for DDIM\n",
    "g_cuda = torch.Generator(device='cuda').manual_seed(52362)\n",
    "\n",
    "num_samples = 10\n",
    "guidance_scale = 7\n",
    "num_inference_steps = 70\n",
    "height = 512\n",
    "width = 512"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Play around with these parameters.\n",
    "- Note: If you want to be in the image you have to add your abbreviation and gender into the positive promt.\n",
    "- Run the next two cells to generate the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example prompt 1\n",
    "prompt = f\"photo of face and shoulders of {YOUR_NAME_ABBREVIATION} {MAN_OR_WOMAN} in a blue suit from the front, front view, closeup, centered frame, symmetric, studio lighting, clear and realistic face, uhd faces, pexels, 85mm, casual pose, 35mm film roll photo, hard light, detailed skin texture, masterpiece, sharp focus, pretty, lovely, adorable, attractive, hasselblad, candid street portrait\"\n",
    "negative_prompt = \"blender, ugly, multiple hands, bad anatomy, bad proportions, unrealistic, full body, cropped, lowres, poorly drawn face, out of frame, poorly drawn hands, double, blurred, disfigured, deformed, repetitive, black and white \"\n",
    "\n",
    "# Example prompt 2\n",
    "#prompt = f\"photo of face and shoulders of {YOUR_NAME_ABBREVIATION} {MAN_OR_WOMAN} in a blue suit from the front, front view, closeup, centered frame, symmetric, studio lighting, clear and realistic face, uhd faces, pexels, 85mm, casual pose, 35mm film roll photo, hard light, detailed skin texture, masterpiece, sharp focus, pretty, lovely, adorable, attractive, hasselblad, candid street portrait\"\n",
    "#negative_prompt = \"blender, ugly, multiple hands, bad anatomy, bad proportions, unrealistic, full body, cropped, lowres, poorly drawn face, out of frame, poorly drawn hands, double, blurred, disfigured, deformed, repetitive, black and white \"\n",
    "\n",
    "# Example prompt 3\n",
    "#prompt = f\"photo of face and shoulders of {YOUR_NAME_ABBREVIATION} {MAN_OR_WOMAN} in a blue suit from the front, front view, closeup, centered frame, symmetric, studio lighting, clear and realistic face, uhd faces, pexels, 85mm, casual pose, 35mm film roll photo, hard light, detailed skin texture, masterpiece, sharp focus, pretty, lovely, adorable, attractive, hasselblad, candid street portrait\"\n",
    "#negative_prompt = \"blender, ugly, multiple hands, bad anatomy, bad proportions, unrealistic, full body, cropped, lowres, poorly drawn face, out of frame, poorly drawn hands, double, blurred, disfigured, deformed, repetitive, black and white \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = pipe(\n",
    "    prompt,\n",
    "    height=height,\n",
    "    width=width,\n",
    "    negative_prompt=negative_prompt,\n",
    "    num_images_per_prompt=num_samples,\n",
    "    num_inference_steps=num_inference_steps,\n",
    "    guidance_scale=guidance_scale,\n",
    "    generator=g_cuda\n",
    ").images\n",
    "for image in images:\n",
    "  display(image)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
